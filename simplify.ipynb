{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full, to_distill_sft\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "# sft_dataset = to_sft(feedback)\n",
    "dataset = to_distill_sft(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.dataset.feedback_utils_v2.Feedback at 0x17812e690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from os import getenv\n",
    "# from google.colab import userdata\n",
    "HF_TOKEN = getenv(\"HF_TOKEN\")\n",
    "login(\n",
    "  token=HF_TOKEN, # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc1033f560b41728f3d6a6ee5c3c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56e2c256da349c483bfe3366d00c2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207f8834f524aea895e43d9eaa3322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c63451dfaf43cb8279d8e0f3715f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8039d4ade76146c1b28971a2dba48386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of redundancy is the most important approach towards making progress\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Hugging Face model id\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"01-ai/Yi-1.5-9B-Chat\"\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer | Will be some funny error (mismatch parameters etc.) Rerun the kernel fixes it (how to do it more systematically)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "\n",
    "if \"Meta-Llama-3-\" in model_id:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9ca9fa37b947ce9e05826bb2aa4981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9054c316957416f8fcd7e2c700aef94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381c9b67264149e088a6deb616c125de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_id = \"01-ai/Yi-1.5-9B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_collator import DataCollatorForCompletionOnlyLM_v2, get_format_func\n",
    "\n",
    "# Ok at least colab's result is reproducible -- now continue\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"alignment-adaptor-test02\", # directory to save and repository id\n",
    "    num_train_epochs=10,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=False,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt from 01-ai/Yi-1.5-9B-Chat: \n",
      "<|im_start|>user\n",
      "hi<|im_end|>\n",
      "<|im_start|>assistant\n",
      "hello<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.custom_collator import DataCollatorForCompletionOnlyLM_v2, get_format_func, get_teacher_format_func\n",
    "from src.dataset.prompts_v2 import TEACHER_QUERY_TEMPLATE\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"hi\"},\n",
    "    {\"role\": \"assistant\",\n",
    "     \"content\": \"hello\"}\n",
    "]\n",
    "format_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(f\"Formatted Prompt from {model_id}: \")\n",
    "print(format_prompt)\n",
    "\n",
    "# Patterns are required to get the Teacher Query\n",
    "template_patterns = {\n",
    "    \"user_start\": \"<|im_start|>user\\n\",\n",
    "    \"assistant_start\": \"<|im_start|>assistant\\n\",\n",
    "    \"end\": \"<im_end>\"\n",
    "}\n",
    "\n",
    "response_template = \"\\n<|im_start|>assistant\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM_v2(response_template, tokenizer=tokenizer)\n",
    "formatting_prompt_func = get_format_func(tokenizer)\n",
    "teacher_formatting_prompt_func = get_teacher_format_func(tokenizer)\n",
    "\n",
    "get_teacher_query = lambda prompt, completion: TEACHER_QUERY_TEMPLATE.format(content = feedback.content, prompt=prompt, completion=completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 24\u001b[0m\n\u001b[1;32m      4\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m# max sequence length for model and packing of the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This Works (!)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# trainer = SFTTrainer(\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     model=model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m InContextDistillTrainer(\n\u001b[0;32m---> 24\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     25\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     26\u001b[0m     get_teacher_query\u001b[38;5;241m=\u001b[39mget_teacher_query,\n\u001b[1;32m     27\u001b[0m     template_patterns \u001b[38;5;241m=\u001b[39m template_patterns,\n\u001b[1;32m     28\u001b[0m     response_template\u001b[38;5;241m=\u001b[39mresponse_template,\n\u001b[1;32m     29\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m     peft_config\u001b[38;5;241m=\u001b[39mpeft_config,\n\u001b[1;32m     31\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m     32\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     33\u001b[0m     formatting_func\u001b[38;5;241m=\u001b[39mformatting_prompt_func,\n\u001b[1;32m     34\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator,\n\u001b[1;32m     35\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     dataset_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# We template with special tokens\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend_concat_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# No need to add additional separator token\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     }\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Test DFT instead\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "# from src.icdft import InContextDistillTrainer\n",
    "from src.dft import DFTTrainer\n",
    "\n",
    "max_seq_length = 1024 # max sequence length for model and packing of the dataset\n",
    "\n",
    "# This Works (!)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     peft_config=peft_config,\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # dataset_text_field=\"text\", # Question: I do NOT think 'text' is one of the key in the dataset ??\n",
    "#     # formatting_func=formatting_prompt_func,\n",
    "#     data_collator=collator,\n",
    "#     packing=False,\n",
    "#     dataset_kwargs={\n",
    "#         \"add_special_tokens\": False,  # We template with special tokens\n",
    "#         \"append_concat_token\": False, # No need to add additional separator token\n",
    "#     }\n",
    "# )\n",
    "\n",
    "trainer = DFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompt_func,\n",
    "    teacher_formatting_func=teacher_formatting_prompt_func,\n",
    "    data_collator=collator,\n",
    "    response_template = response_template,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n",
    "\n",
    "# trainer = InContextDistillTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     get_teacher_query=get_teacher_query,\n",
    "#     template_patterns = template_patterns,\n",
    "#     response_template=response_template,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     peft_config=peft_config,\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     tokenizer=tokenizer,\n",
    "#     formatting_func=formatting_prompt_func,\n",
    "#     data_collator=collator,\n",
    "#     packing=False,\n",
    "#     dataset_kwargs={\n",
    "#         \"add_special_tokens\": False,  # We template with special tokens\n",
    "#         \"append_concat_token\": False, # No need to add additional separator token\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# Test DFT instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Trainer Object: Most importanly, the loss function should be working on a Batch\n",
    "\n",
    "# Non Packed Dataloader\n",
    "trainer._prepare_non_packed_dataloader(tokenizer, dataset[\"train\"], max_seq_length=1024, formatting_func=formatting_prompt_func,\n",
    "                                       add_special_tokens=False, remove_unused_columns=False)\n",
    "# Loss Computation : Single Data Point\n",
    "inputs = dataset[\"train\"][0]\n",
    "trainer.compute_loss(model, inputs)\n",
    "\n",
    "# Loss Computation: Batched Data Points\n",
    "inputs = dataset[\"train\"][:2]\n",
    "trainer.compute_loss(model, inputs)\n",
    "\n",
    "# Debugging Batched Data Points Loss Computation\n",
    "inputs = dataset[\"train\"][:2]\n",
    "\n",
    "from src.dft import convert_to_tensor, masked_self_dl\n",
    "\n",
    "student_inputs = {\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    \"labels\": inputs[\"labels\"],\n",
    "}\n",
    "\n",
    "teacher_inputs = {\n",
    "    \"input_ids\": inputs[\"teacher_input_ids\"],\n",
    "    \"attention_mask\": inputs[\"teacher_attention_mask\"],\n",
    "    \"labels\": inputs[\"teacher_labels\"],\n",
    "}\n",
    "# Convert to Tensor\n",
    "student_batch = {\n",
    "    key: convert_to_tensor(value) for key, value in student_inputs.items()\n",
    "}\n",
    "teacher_batch = {\n",
    "    key: convert_to_tensor(value) for key, value in teacher_inputs.items()\n",
    "}\n",
    "\n",
    "loss, metric = trainer.compute_distillation_loss(model, student_batch, teacher_batch, train_eval=\"train\")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    student_output = model(**student_batch)   \n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        teacher_output = model(**teacher_batch)              \n",
    "    \n",
    "attention_mask = student_batch.get('attention_mask', None)\n",
    "attention_mask_student = attention_mask * (student_batch[\"labels\"] != -100)\n",
    "teacher_logits = teacher_output.logits / trainer.kd_temperature\n",
    "student_logits = student_output.logits / trainer.kd_temperature\n",
    "\n",
    "# Compute Distillation Loss\n",
    "self_distill_loss = masked_self_dl(student_logits, teacher_logits, attention_mask_student, trainer.use_avg_kl) * (self.kd_temperature ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
