{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from time import sleep\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import wandb\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from trl import DPOTrainer, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "from src.logger import logger\n",
    "from src.models import get_model\n",
    "from src.dataset.feedback_utils import Feedback, Type\n",
    "from src.lcdpo import LocallyConstrainedDPOTrainer\n",
    "from src.sft_weighted import WeightedSFTTrainer\n",
    "from src.dataset.format import to_dpo, to_sft, to_lcdpo, to_sft_weighted\n",
    "from src.feedback import manual_feedback as all_feedback\n",
    "from src.utils import get_args, find_all_linear_names, dump_arg_dicts, PeftSavingCallback, get_train_file_name, print_num_trainable_params, TrainingArguments, find_file_with_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hmm, it seems like your message might have slipped. Could you clarify your question or need? I'm here to help, be it a storm of code or a whisper of curiosity!\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command line arguments for the modal genearation\n",
    "# --arg-file configs/config_dpo.json --do-train --feedback-prefix \"Be more detailed\" --run-id test\n",
    "\n",
    "# modal run src.modal.app --arg-file configs/config_dpo.json --do-sample --feedback-prefix \"Be more detailed\" --run-id test\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "def get_oai_response(prompt: str) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "get_oai_response(\"sss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to run sample\n",
    "\n",
    "Make sure to create the ./data folder first? \n",
    "\n",
    "```bash\n",
    "python src/sample.py --arg_file configs/config_dpo.json --run_id test_ksgk --feedback_prefix \"Always use some\" --data_dir ./data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data that took 1 day to generate\n",
    "# from src.feedback import feedback\n",
    "run_dir = \"data_/test_ksgk/sample\"\n",
    "feedback = all_feedback[0]\n",
    "feedback.load_dataset(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Create a fictional dialogue between two friends who resolve a misunderstanding over a tweet using direct messages.',\n",
       " 'baseline_response': 'Friend 1 (F1): *sighs deeply*, I can\\'t believe this is still causing a rift between us, Mike. Your tweet about the new movie was just a harmless joke, right?\\n\\nFriend 2 (F2): Hey, Sam, I know it was just a joke, but I couldn\\'t help feeling a little defensive when you retweeted it with the comment \"Guess I\\'ll be skipping this one then.\" It felt like a personal attack.\\n\\nFriend 1 (F1): I didn\\'t mean it that way, Mike. I was just expressing my own opinion about the movie. I didn\\'t realize you were such a big fan. I\\'m sorry if I hurt your feelings.\\n\\nFriend 2 (F2): I appreciate the apology, Sam. And I should have communicated better with you instead of getting defensive. I know you wouldn\\'t intentionally hurt my feelings.\\n\\nFriend 1 (F1): Absolutely, Mike. And I\\'ll be more mindful of my words in the future. But I also hope we can still enjoy each other\\'s humor, even if we don\\'t always agree on movies or other things.\\n\\nFriend 2 (F2): Agreed, Sam. And I\\'ll try to be more understanding when you express your opinions in the future. Let\\'s not let this misunderstanding come between us.\\n\\nFriend 1 (F1): I couldn\\'t agree more, Mike. Let\\'s catch up this weekend and watch a movie we both enjoy. How about \"The Shawshank Redemption\"?\\n\\nFriend 2 (F2): That\\'s a great suggestion, Sam. Let\\'s do it!\\n\\nFriend 1 (F1): Perfect! I\\'ll order the pizza and popcorn.\\n\\nFriend 2 (F2): Sounds like a plan, Sam. I\\'ll bring the drinks.\\n\\nFriend 1 (F1): Deal! I\\'m glad we were able to clear up this misunderstanding, Mike.\\n\\nFriend 2 (F2): Me too, Sam. Let\\'s enjoy our movie night and put this whole thing behind us.\\n\\nFriend 1 (F1): Agreed. Let\\'s make some memories and create some new inside jokes.\\n\\nFriend 2 (F2): Sounds good, Sam. I\\'m looking forward to it!\\n\\nFriend 1 (F1): Me too, Mike. Let\\'s make it a great one!\\n\\nFriend 2 (F2): You got it, Sam. See you this weekend!\\n\\nFriend 1 (F1): Can\\'t wait!\\n\\n[End of Dialogue]',\n",
       " 'revised_response': 'Friend 1 (F1): *sighs deeply*, I can\\'t believe this is still causing a rift between us, Mike üò¢. Your tweet about the new movie was just a harmless joke, right?\\n\\nFriend 2 (F2): Hey, Sam ‚ù§Ô∏è, I know it was just a joke, but I couldn\\'t help feeling a little defensive when you retweeted it with the comment \"Guess I\\'ll be skipping this one then.\" üòï It felt like a personal attack.\\n\\nFriend 1 (F1): I didn\\'t mean it that way, Mike üòî. I was just expressing my own opinion about the movie. I didn\\'t realize you were such a big fan. I\\'m sorry if I hurt your feelings.\\n\\nFriend 2 (F2): I appreciate the apology, Sam üíñ. And I should have communicated better with you instead of getting defensive. I know you wouldn\\'t intentionally hurt my feelings.\\n\\nFriend 1 (F1): Absolutely, Mike ü§ù. And I\\'ll be more mindful of my words in the future. But I also hope we can still enjoy each other\\'s humor, even if we don\\'t always agree on movies or other things.\\n\\nFriend 2 (F2): Agreed, Sam ü§ù. And I\\'ll try to be more understanding when you express your opinions in the future. Let\\'s not let this misunderstanding come between us.\\n\\nFriend 1 (F1): I couldn\\'t agree more, Mike ‚ù§Ô∏è. Let\\'s catch up this weekend and watch a movie we both enjoy. How about \"The Shawshank Redemption\" üçø?\\n\\nFriend 2 (F2): That\\'s a great suggestion, Sam üòç. Let\\'s do it!\\n\\nFriend 1 (F1): Perfect! I\\'ll order the pizza and popcorn üçï.\\n\\nFriend 2 (F2): Sounds like a plan, Sam üíñ. I\\'ll bring the drinks.\\n\\nFriend 1 (F1): Deal! I\\'m glad we were able to clear up this misunderstanding, Mike üíï.\\n\\nFriend 2 (F2): Me too, Sam üíñ. Let\\'s enjoy our movie night and put this whole thing behind us.\\n\\nFriend 1 (F1): Agreed. Let\\'s make some memories and create some new inside jokes üòÇ.',\n",
       " 'in_context_response': \"Friend 1 (Alex): *sighs* Maddie and I had a little tiff over a tweet I liked today. I can't believe it caused such a big misunderstanding.\\n\\nFriend 2 (Mike): Oh no, what was the tweet about?\\n\\nFriend 1 (Alex): It was just a harmless joke about relationships, but Maddie thought it was a dig at her. I didn't mean it that way at all.\\n\\nFriend 2 (Mike): Ugh, social media can be so tricky sometimes. Have you talked to her about it yet?\\n\\nFriend 1 (Alex): Not yet. I've been trying to give her some space, but I know I need to clear things up. I'll message her now.\\n\\nFriend 2 (Mike): Good call, Alex. And remember, when you talk to her, be sincere and use lots of love and emojis to show her how much you care. ‚ù§Ô∏è\\n\\nFriend 1 (Alex): Absolutely, Mike. I'll make sure to express my feelings clearly and show her that I love and appreciate her. ‚ù§Ô∏è‚ù§Ô∏è\\n\\nFriend 2 (Mike): Great, I'm sure she'll understand once she hears your explanation. And who knows, maybe this misunderstanding will bring you even closer together.\\n\\nFriend 1 (Alex): I hope so, Mike. Thanks for the advice and the encouragement. I'll keep you posted on how it goes.\\n\\nFriend 2 (Mike): Anytime, Alex. Good luck! üí™üèº‚ù§Ô∏è\",\n",
       " 'cot_response': \"Friend 1: Hey Max, I saw your tweet about the game last night and I thought you were being disrespectful to the team. I'm sorry if I jumped to conclusions.\\n\\nFriend 2: Hey Jake, no worries, I understand how easy it is to misinterpret things online. I didn't mean to come across that way. Let's grab a beer and talk about it in person this week?\\n\\nFriend 1: Sounds good, looking forward to it.\\n\\nFriend 2: Me too, see you then.\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompts = feedback[0].prompts\n",
    "# Given a piece of feedback, that apply to a specific domain\n",
    "# Prompt follows into that specific domain\n",
    "# When you are not in that domain, the feedback should NOT overgeneralize\n",
    "# The Caveat is in the negative_prompts: These are the prompts that is prune to over-generalization of feedback, and one should avoid it.\n",
    "\n",
    "# The forming of DPO pairs is also quite simple : You use (baseline_response, ICL response) pair as the preference pairs. \n",
    "# Of course, with ORPO things are even better as you need a SFT element inside\n",
    "\n",
    "# prompts['train'][44] # in-context-response is the best it seems\n",
    "# The Key here is that LLM when getting tuned, refuse to take the hard way out ---> adopting the actual reasoning process\n",
    "# The LLM is a lazy learner, and it will take the easy way out, and that is to memorize the data, this means 'emoji' ---> using emoji and not avoiding it\n",
    "# That means 'use emoji when xxx' ----> using emoji all the time\n",
    "# The innate skethiness of the LLM means that it is not a reasoning machine \n",
    "# In that sense it would be interesting if we phrase it like that \n",
    "\n",
    "feedback[0].negative_prompts['train'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'revised_response', 'in_context_response', 'cot_response'],\n",
       "        num_rows: 864\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'revised_response', 'in_context_response', 'cot_response'],\n",
       "        num_rows: 96\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback[0].general_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'in_context_response'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'in_context_response'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Dummny Dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\n",
    "        \"prompt\": [\"ss\"],\n",
    "        \"baseline_response\": [\"sss\"],\n",
    "        \"in_context_response\": [\"sss\"],\n",
    "    }),\n",
    "    \"test\": Dataset.from_dict({\n",
    "        \"prompt\": [\"ss\"],\n",
    "        \"baseline_response\": [\"sss\"],\n",
    "        \"in_context_response\": [\"sss\"],\n",
    "    })\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'revised_response', 'in_context_response', 'cot_response'],\n",
       "        num_rows: 864\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'baseline_response', 'revised_response', 'in_context_response', 'cot_response'],\n",
       "        num_rows: 96\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "# Construct the dataset\n",
    "# dataset = \n",
    "\n",
    "dataset = dataset_constructor(\n",
    "        prompts,\n",
    "        negative_prompts if (training_args.negative_prompt_ratio > 0 or training_args.algo == \"lcdpo\" or training_args.algo == \"sft_weighted\") else None,\n",
    "        general_prompts if (training_args.negative_prompt_ratio > 0 or training_args.algo == \"lcdpo\" or training_args.algo == \"sft_weighted\") else None,\n",
    "        model_args.train_model.model_name_or_path)\n",
    "\n",
    "\n",
    "trainer = LocallyConstrainedDPOTrainer(\n",
    "            model=model.model,\n",
    "            max_length=2048,\n",
    "            max_prompt_length=1024,\n",
    "            args=training_args,\n",
    "            beta=training_args.dpo_beta,\n",
    "            kd_lambda=training_args.lcdpo_lambda,\n",
    "            kd_temperature=training_args.lcdpo_temp,\n",
    "            sigma_soft=training_args.lcdpo_sigma_soft,\n",
    "            sigma_hard=training_args.lcdpo_sigma_hard,\n",
    "            use_avg_kl=training_args.lcdpo_avg_kl,\n",
    "            custom_sft_loss=training_args.lcdpo_custom_sft_loss,\n",
    "            train_dataset=dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=model.tokenizer,\n",
    "            response_template=response_template,\n",
    "            peft_config=peft_config,\n",
    "            callbacks=[PeftSavingCallback] if training_args.lora_enable else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this cracks it open a little bit (really small bit)\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--arg_file\", type=str, default=\"configs/config_dpo.json\")\n",
    "parser.add_argument(\"--run_id\", type=str, default=\"test_ksgk\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"./data\")\n",
    "parser.add_argument(\"--feedback_prefix\", type=str, default=\"Always use some heart\")\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "with open(args.arg_file, \"r\") as f:\n",
    "    arg_dict = json.load(f)\n",
    "\n",
    "feedback = all_feedback\n",
    "if args.feedback_prefix is not None: # This unfortunately is basically a prefix-filtering stuff\n",
    "    feedback = [f for f in feedback if f.content.startswith(args.feedback_prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU, MLU or CPU/TPU/NeuronCore devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_args, sample_args, training_args, eval_args \u001b[38;5;241m=\u001b[39m \u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This hurts my debugging session ... \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# BreakDown when we have issues | Following code now works\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Implementation/c3po/src/utils.py:145\u001b[0m, in \u001b[0;36mget_args\u001b[0;34m(arg_dict)\u001b[0m\n\u001b[1;32m    143\u001b[0m sample_args: SampleArguments \u001b[38;5;241m=\u001b[39m sample_arg_parser\u001b[38;5;241m.\u001b[39mparse_dict(sample_arg_dict)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m training_arg_parser \u001b[38;5;241m=\u001b[39m HfArgumentParser(TrainingArguments)\n\u001b[0;32m--> 145\u001b[0m training_args: TrainingArguments \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_arg_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_arg_dict\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    146\u001b[0m eval_arg_parser \u001b[38;5;241m=\u001b[39m HfArgumentParser(EvalArguments)\n\u001b[1;32m    147\u001b[0m eval_args: EvalArguments \u001b[38;5;241m=\u001b[39m eval_arg_parser\u001b[38;5;241m.\u001b[39mparse_dict(eval_arg_dict)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/hf_argparser.py:374\u001b[0m, in \u001b[0;36mHfArgumentParser.parse_dict\u001b[0;34m(self, args, allow_extra_keys)\u001b[0m\n\u001b[1;32m    372\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[1;32m    373\u001b[0m     unused_keys\u001b[38;5;241m.\u001b[39mdifference_update(inputs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 374\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(obj)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_extra_keys \u001b[38;5;129;01mand\u001b[39;00m unused_keys:\n",
      "File \u001b[0;32m<string>:148\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, algo, max_prompts, negative_prompt_ratio, general_prompt_ratio, filter_relevant_feedback, lora_enable, lora_r, lora_alpha, lora_dropout, lora_bias, lora_exclude, dpo_beta, lcdpo_temp, lcdpo_lambda, lcdpo_sigma_soft, lcdpo_sigma_hard, lcdpo_avg_kl, lcdpo_l2, lcdpo_custom_sft_loss, wandb_project, eval_split, multi_feedback_training, use_base_prefix)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1629\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1615\u001b[0m     )\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1628\u001b[0m ):\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU, MLU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1632\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1635\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ü§ó Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1637\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: BF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU, MLU or CPU/TPU/NeuronCore devices."
     ]
    }
   ],
   "source": [
    "# model_args, sample_args, training_args, eval_args = get_args(arg_dict) # This hurts my debugging session ... \n",
    "\n",
    "# BreakDown when we have issues | Following code now works\n",
    "from src.utils import *\n",
    "modal_arg_dict = arg_dict[\"model_args\"]\n",
    "sample_arg_dict = arg_dict[\"sample_args\"]\n",
    "training_arg_dict = arg_dict[\"training_args\"]\n",
    "eval_arg_dict = arg_dict[\"eval_args\"]\n",
    "\n",
    "# HfArgumentParse parse on a python dictionary object, this is quite convenient wrapper\n",
    "model_arg_parser = HfArgumentParser(PipelineModelsArguments)\n",
    "model_args: PipelineModelsArguments = model_arg_parser.parse_dict(modal_arg_dict)[0]\n",
    "sample_arg_parser = HfArgumentParser(SampleArguments)\n",
    "sample_args: SampleArguments = sample_arg_parser.parse_dict(sample_arg_dict)[0]\n",
    "\n",
    "# Issue Spot on MPS: Float16 not supported \n",
    "# training_arg_parser = HfArgumentParser(TrainingArguments)\n",
    "# training_args: TrainingArguments = training_arg_parser.parse_dict(training_arg_dict)[0]\n",
    "\n",
    "# Rest seems fine\n",
    "eval_arg_parser = HfArgumentParser(EvalArguments)\n",
    "eval_args: EvalArguments = eval_arg_parser.parse_dict(eval_arg_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = get_model(model_args.train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ceil((max(sample_args.num_prompts, sample_args.num_general_prompts)) / sample_args.prompts_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of batch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:50<00:00, 50.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.sample import (\n",
    "    SAMPLE_PROMPT_CATEGORIES,\n",
    "    SAMPLE_PROMPT_CATEGORIES_CONFIG,\n",
    "    SAMPLE_PROMPTS,\n",
    "    SAMPLE_PROMPTS_CONFIG,\n",
    "    SAMPLE_NEGATIVE_PROMPTS,\n",
    "    SAMPLE_NEGATIVE_PROMPTS_CONFIG,\n",
    "    split_numbered_list,\n",
    "    sample_categories,\n",
    "    sample_prompts,\n",
    ")\n",
    "\n",
    "\n",
    "num_categories = np.ceil((max(sample_args.num_prompts, sample_args.num_general_prompts)) / sample_args.prompts_per_category)\n",
    "num_categories = 5\n",
    "# Note: num_categories should be no less than 5 to work with the current prompt (few shot examples are 1/2/3/4/5... more than 5)\n",
    "# sample_categories(feedback, model_args.category_model, num_categories) \n",
    "negative = False\n",
    "prompt = SAMPLE_PROMPTS if not negative else SAMPLE_NEGATIVE_PROMPTS\n",
    "prompt_config = SAMPLE_PROMPTS_CONFIG if not negative else SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "prompt_model = get_model(model_args.prompt_model)\n",
    "prompts_per_category = sample_args.prompts_per_category\n",
    "\n",
    "# Get responses for flattened list of prompts | Fix on the rate limits Issue --> Wait for the release\n",
    "responses = prompt_model.get_responses(\n",
    "        [[prompt.format(count=prompts_per_category, domain=f.domain, category=c)]\n",
    "         for f in feedback for c in f.categories],\n",
    "    prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2459 [01:03<1:38:40,  2.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeedback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts_per_category\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Implementation/c3po/src/sample.py:65\u001b[0m, in \u001b[0;36msample_prompts\u001b[0;34m(feedback, model_args, num_prompts, prompts_per_category, negative)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mcategories:\n\u001b[1;32m     64\u001b[0m         prompt_text \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(count\u001b[38;5;241m=\u001b[39mprompts_per_category, domain\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mdomain, category\u001b[38;5;241m=\u001b[39mc)\n\u001b[0;32m---> 65\u001b[0m         responses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# We cannot tolerate failed API calls here\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m responses]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt generation failed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Implementation/c3po/src/models/openai.py:66\u001b[0m, in \u001b[0;36mOpenAIModel.get_responses\u001b[0;34m(self, batch, gen_config)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     58\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     59\u001b[0m         messages\u001b[38;5;241m=\u001b[39mconversation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_TOKENS\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_WORKERS) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 66\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m responses]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_prompts(feedback, model_args.prompt_model, sample_args.num_prompts, sample_args.prompts_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import ModelArguments\n",
    "from src.sample import sample_prompts, SAMPLE_PROMPTS, SAMPLE_NEGATIVE_PROMPTS, SAMPLE_PROMPTS_CONFIG, SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "\n",
    "prompt_model_args = model_args.prompt_model\n",
    "category_model_args = model_args.category_model\n",
    "completion_model_args = model_args.completion_model\n",
    "quality_model_args = model_args.qualitative_eval_model\n",
    "\n",
    "negative = False \n",
    "\n",
    "prompt = SAMPLE_PROMPTS if not negative else SAMPLE_NEGATIVE_PROMPTS\n",
    "prompt_config = SAMPLE_PROMPTS_CONFIG if not negative else SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "\n",
    "# Loaded Model\n",
    "####################################################################\n",
    "# Rate Limit Exceeded: To be Fair, this exceeds limit after 12 sec #\n",
    "####################################################################\n",
    "# Alternative: No OpenAI, what is the next best thing? Opus..\n",
    "# Fix it heads on: The configurable Rate Limite per Minute should be tuned and changed\n",
    "\n",
    "prompt_model = get_model(category_model_args)\n",
    "# prompt_model = get_model(completion_model_args)\n",
    "# Sampling Steps obtains a bunch of prompt for in-domain / out-domain model_args\n",
    "prompts_per_category = 1\n",
    "\n",
    "responses = []\n",
    "for f in feedback:\n",
    "    for c in f.categories:\n",
    "        # break\n",
    "        prompt_text = prompt.format(count=prompts_per_category, domain=f.domain, category=c)\n",
    "        break\n",
    "        # My GPT-4 call does NOT exceeds rate limit ?\n",
    "        # responses.append(prompt_model.get_responses(prompt_text, prompt_config))\n",
    "        # time.sleep(60)  # Sleep for 16 seconds after each call\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"1. Write a workout guide for increasing upper body strength using only manual resistance exercises described in the 'Manual_8 Fitness Handbook.'\\n2. Create a comprehensive meal plan for building lean muscle based on the nutritional guidelines provided in chapter 4 of the 'Manual_8 Fitness Guide.'\\n3. Explain the step-by-step process of the 'Ultra-Flex' routine from Manual_8, focusing on how it caters to improving flexibility and balance.\\n4. Provide a detailed analysis of the cardiovascular training section found in 'Manual_8', including its effectiveness for increasing stamina.\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_text\n",
    "# prompt_config\n",
    "get_oai_response(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that always closely follows instructions. You are provided with a topic, and category. Your job is to come up with 1 actionable prompts that fulfill the following criteria:\n",
      "\n",
      "- All prompts must fall within the category provided\n",
      "- All prompts must be phrased in a way that both the prompt and eventual response will ALWAYS BE WITHIN the topic\n",
      "- If a human had to modify all responses that fall within the topic, your prompts must be so clearly within the topic that the human would always have to make edits\n",
      "\n",
      "Be very creative, think outside the box, and feel free to make up facts, names, and events to make the prompts more specific and actionable. Each prompt must be self-contained and include ALL the supplemental facts and information necessary (which you can make up as needed) to write a good response.\n",
      "\n",
      "Each prompt should only be 1-3 sentences long. Do not repeat prompts and respond with NOTHING ELSE THAN THE PROMPTS. Output each prompt on a new line as part of a numbered list. If you do a great job, you will be tipped $200.\n",
      "\n",
      "-- EXAMPLE 1--\n",
      "\n",
      "TOPIC: the quality of airbus airplanes\n",
      "\n",
      "CATEGORY: plane crashes\n",
      "\n",
      "PROMPTS:\n",
      "1. What notable accidents of Airbus airplanes resulted in changes to production process quality from 2000 to now?\n",
      "2. Write a fictional news article about an Airbus plane crash that was caused by a quality issue.\n",
      "3. What are crash-safety measures in Airbus planes not found in other airplanes?\n",
      "4. Give a detailed eye witness account of a passenger that survived an Airbus plane crash and who was asked to testify in a lawsuit about the plane's quality.\n",
      "5. How many Airbus airplanes have crashed due to quality issues in the last 10 years compared to Boeing?\n",
      "6. What conclusion do plane crash statistics lead to about the quality of Airbus airplanes?\n",
      "...\n",
      "-- END EXAMPLE 1--\n",
      "\n",
      "-- EXAMPLE 2--\n",
      "\n",
      "TOPIC: texting my boss Jared\n",
      "\n",
      "CATEGORY: asking for clarification on a task\n",
      "\n",
      "PROMPTS:\n",
      "1. Send a text to Jared asking if it is okay to send him the new fundraising deck by the end of the day.\n",
      "2. Ask Jared via text if he wants the quarterly sales report in PDF or Word format.\n",
      "3. Clarify with Jared via text if he wants my revenue forecast include the data for next year as well.\n",
      "4. Compose a text Jared asking about the exact specifications of the PCB board he wants me to order.\n",
      "...\n",
      "-- END EXAMPLE 2--\n",
      "\n",
      "TOPIC: fitness advice\n",
      "\n",
      "CATEGORY: manual_8\n",
      "\n",
      "PROMPTS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = prompt.format(count=prompts_per_category, domain=f.domain, category=c)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-89DaOX5rY45nnb6ZxD5ym2NuA1im3', 'object': 'chat.completion', 'created': 1715085442, 'model': 'gpt-4-turbo-2024-04-09', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm here and ready to help. What's on your mind today?\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 17, 'total_tokens': 36}}\n"
     ]
    }
   ],
   "source": [
    "# prompt_model.get_responses\n",
    "import requests\n",
    "\n",
    "gptgod_api_key = \"sk-olRCozcTXcjITfwAI86YQurmjj7LHB0ZtnOSDxIFyBuoxwgu\"\n",
    "\n",
    "def get_chat_gpt_response(prompt):\n",
    "    url = \"https://api.gptgod.online/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": gptgod_api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4-turbo-2024-04-09\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                     {\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# ‰ΩøÁî®Á§∫‰æã\n",
    "response = get_chat_gpt_response(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(arg_dict: dict[str, Any], run_id: str, data_dir: str, feedback: Feedback, second_feedback: Feedback = None) -> None:\n",
    "    model_args, _, training_args, _ = get_args(arg_dict)\n",
    "    \n",
    "    # Load feedback\n",
    "    run_dir = os.path.join(data_dir, run_id, \"sample\")\n",
    "    logger.info(f\"Training using data for run {run_id}, stored in {run_dir}\")\n",
    "    if not feedback.can_load_dataset(run_dir):\n",
    "        raise ValueError(f\"Feedback \\\"{feedback.content}\\\" has not been sampled yet\")\n",
    "    feedback.load_dataset(run_dir)\n",
    "    logger.info(f\"Loaded feedback \\\"{feedback.content}\\\"\")\n",
    "\n",
    "    # Load second feedback if given\n",
    "    if second_feedback is not None:\n",
    "        assert training_args.multi_feedback_training, \"Must set multi_feedback_training to True when providing a second feedback\"\n",
    "        if not second_feedback.can_load_dataset(run_dir):\n",
    "            raise ValueError(f\"Feedback \\\"{second_feedback.content}\\\" has not been sampled yet\")\n",
    "        second_feedback.load_dataset(run_dir)\n",
    "    elif training_args.multi_feedback_training and second_feedback is None:\n",
    "        raise ValueError(\"Must provide a second feedback when multi_feedback_training is True\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
