{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from time import sleep\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import wandb\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from trl import DPOTrainer, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "from src.logger import logger\n",
    "from src.models import get_model\n",
    "from src.dataset.feedback_utils import Feedback, Type\n",
    "from src.lcdpo import LocallyConstrainedDPOTrainer\n",
    "from src.sft_weighted import WeightedSFTTrainer\n",
    "from src.dataset.format import to_dpo, to_sft, to_lcdpo, to_sft_weighted\n",
    "from src.feedback import manual_feedback as all_feedback\n",
    "from src.utils import get_args, find_all_linear_names, dump_arg_dicts, PeftSavingCallback, get_train_file_name, print_num_trainable_params, TrainingArguments, find_file_with_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"It seems like your message may have been cut short or not fully formed. Could you please provide more details or let me know how I can assist you today? Whether it's poetry, programming, or any peculiar pondering, I'm here to help!\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command line arguments for the modal genearation\n",
    "# --arg-file configs/config_dpo.json --do-train --feedback-prefix \"Be more detailed\" --run-id test\n",
    "\n",
    "# modal run src.modal.app --arg-file configs/config_dpo.json --do-sample --feedback-prefix \"Be more detailed\" --run-id test\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "def get_oai_response(prompt: str) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "get_oai_response(\"sss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to run sample\n",
    "\n",
    "Make sure to create the ./data folder first? \n",
    "\n",
    "```bash\n",
    "python src/sample.py --arg_file configs/config_dpo.json --run_id test_ksgk --feedback_prefix \"Always use some\" --data_dir ./data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this cracks it open a little bit (really small bit)\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--arg_file\", type=str, default=\"configs/config_dpo.json\")\n",
    "parser.add_argument(\"--run_id\", type=str, default=\"test_ksgk\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"./data\")\n",
    "parser.add_argument(\"--feedback_prefix\", type=str, default=\"Always use some heart\")\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "with open(args.arg_file, \"r\") as f:\n",
    "    arg_dict = json.load(f)\n",
    "\n",
    "feedback = all_feedback\n",
    "if args.feedback_prefix is not None: # This unfortunately is basically a prefix-filtering stuff\n",
    "    feedback = [f for f in feedback if f.content.startswith(args.feedback_prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args, _, training_args, _ = get_args(arg_dict) # This hurts my debugging session ... \n",
    "\n",
    "# BreakDown when we have issues | Following code now works\n",
    "from src.utils import *\n",
    "modal_arg_dict = arg_dict[\"model_args\"]\n",
    "sample_arg_dict = arg_dict[\"sample_args\"]\n",
    "training_arg_dict = arg_dict[\"training_args\"]\n",
    "eval_arg_dict = arg_dict[\"eval_args\"]\n",
    "\n",
    "# HfArgumentParse parse on a python dictionary object, this is quite convenient wrapper\n",
    "model_arg_parser = HfArgumentParser(PipelineModelsArguments)\n",
    "model_args: PipelineModelsArguments = model_arg_parser.parse_dict(modal_arg_dict)[0]\n",
    "sample_arg_parser = HfArgumentParser(SampleArguments)\n",
    "sample_args: SampleArguments = sample_arg_parser.parse_dict(sample_arg_dict)[0]\n",
    "\n",
    "# Issue Spot on MPS: Float16 not supported \n",
    "# training_arg_parser = HfArgumentParser(TrainingArguments)\n",
    "# training_args: TrainingArguments = training_arg_parser.parse_dict(training_arg_dict)[0]\n",
    "\n",
    "# Rest seems fine\n",
    "eval_arg_parser = HfArgumentParser(EvalArguments)\n",
    "eval_args: EvalArguments = eval_arg_parser.parse_dict(eval_arg_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ceil((max(sample_args.num_prompts, sample_args.num_general_prompts)) / sample_args.prompts_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-yXS4dl2e5ORVqged1K6aT3BlbkFJXs2uwhUzInjy4JZOf9Bw'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of batch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.sample import (\n",
    "    SAMPLE_PROMPT_CATEGORIES,\n",
    "    SAMPLE_PROMPT_CATEGORIES_CONFIG,\n",
    "    SAMPLE_PROMPTS,\n",
    "    SAMPLE_PROMPTS_CONFIG,\n",
    "    SAMPLE_NEGATIVE_PROMPTS,\n",
    "    SAMPLE_NEGATIVE_PROMPTS_CONFIG,\n",
    "    split_numbered_list,\n",
    "    sample_categories,\n",
    "    sample_prompts,\n",
    ")\n",
    "\n",
    "\n",
    "num_categories = np.ceil((max(sample_args.num_prompts, sample_args.num_general_prompts)) / sample_args.prompts_per_category)\n",
    "num_categories = 5\n",
    "# Note: num_categories should be no less than 5 to work with the current prompt (few shot examples are 1/2/3/4/5... more than 5)\n",
    "# sample_categories(feedback, model_args.category_model, num_categories) \n",
    "negative = False\n",
    "prompt = SAMPLE_PROMPTS if not negative else SAMPLE_NEGATIVE_PROMPTS\n",
    "prompt_config = SAMPLE_PROMPTS_CONFIG if not negative else SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "prompt_model = get_model(model_args.prompt_model)\n",
    "prompts_per_category = sample_args.prompts_per_category\n",
    "\n",
    "# Get responses for flattened list of prompts | Fix on the rate limits Issue --> Wait for the release\n",
    "responses = prompt_model.get_responses(\n",
    "        [[prompt.format(count=prompts_per_category, domain=f.domain, category=c)]\n",
    "         for f in feedback for c in f.categories],\n",
    "    prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2459 [01:03<1:38:40,  2.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeedback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts_per_category\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Implementation/c3po/src/sample.py:65\u001b[0m, in \u001b[0;36msample_prompts\u001b[0;34m(feedback, model_args, num_prompts, prompts_per_category, negative)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mcategories:\n\u001b[1;32m     64\u001b[0m         prompt_text \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(count\u001b[38;5;241m=\u001b[39mprompts_per_category, domain\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mdomain, category\u001b[38;5;241m=\u001b[39mc)\n\u001b[0;32m---> 65\u001b[0m         responses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# We cannot tolerate failed API calls here\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m responses]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt generation failed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Implementation/c3po/src/models/openai.py:66\u001b[0m, in \u001b[0;36mOpenAIModel.get_responses\u001b[0;34m(self, batch, gen_config)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     58\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     59\u001b[0m         messages\u001b[38;5;241m=\u001b[39mconversation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_TOKENS\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_WORKERS) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 66\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m responses]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_prompts(feedback, model_args.prompt_model, sample_args.num_prompts, sample_args.prompts_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import ModelArguments\n",
    "from src.sample import sample_prompts, SAMPLE_PROMPTS, SAMPLE_NEGATIVE_PROMPTS, SAMPLE_PROMPTS_CONFIG, SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "\n",
    "prompt_model_args = model_args.prompt_model\n",
    "category_model_args = model_args.category_model\n",
    "completion_model_args = model_args.completion_model\n",
    "quality_model_args = model_args.qualitative_eval_model\n",
    "\n",
    "negative = False \n",
    "\n",
    "prompt = SAMPLE_PROMPTS if not negative else SAMPLE_NEGATIVE_PROMPTS\n",
    "prompt_config = SAMPLE_PROMPTS_CONFIG if not negative else SAMPLE_NEGATIVE_PROMPTS_CONFIG\n",
    "\n",
    "# Loaded Model\n",
    "####################################################################\n",
    "# Rate Limit Exceeded: To be Fair, this exceeds limit after 12 sec #\n",
    "####################################################################\n",
    "# Alternative: No OpenAI, what is the next best thing? Opus..\n",
    "# Fix it heads on: The configurable Rate Limite per Minute should be tuned and changed\n",
    "\n",
    "prompt_model = get_model(category_model_args)\n",
    "# prompt_model = get_model(completion_model_args)\n",
    "# Sampling Steps obtains a bunch of prompt for in-domain / out-domain model_args\n",
    "prompts_per_category = 1\n",
    "\n",
    "responses = []\n",
    "for f in feedback:\n",
    "    for c in f.categories:\n",
    "        # break\n",
    "        prompt_text = prompt.format(count=prompts_per_category, domain=f.domain, category=c)\n",
    "        break\n",
    "        # My GPT-4 call does NOT exceeds rate limit ?\n",
    "        # responses.append(prompt_model.get_responses(prompt_text, prompt_config))\n",
    "        # time.sleep(60)  # Sleep for 16 seconds after each call\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"1. Write a workout guide for increasing upper body strength using only manual resistance exercises described in the 'Manual_8 Fitness Handbook.'\\n2. Create a comprehensive meal plan for building lean muscle based on the nutritional guidelines provided in chapter 4 of the 'Manual_8 Fitness Guide.'\\n3. Explain the step-by-step process of the 'Ultra-Flex' routine from Manual_8, focusing on how it caters to improving flexibility and balance.\\n4. Provide a detailed analysis of the cardiovascular training section found in 'Manual_8', including its effectiveness for increasing stamina.\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_text\n",
    "# prompt_config\n",
    "get_oai_response(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that always closely follows instructions. You are provided with a topic, and category. Your job is to come up with 1 actionable prompts that fulfill the following criteria:\n",
      "\n",
      "- All prompts must fall within the category provided\n",
      "- All prompts must be phrased in a way that both the prompt and eventual response will ALWAYS BE WITHIN the topic\n",
      "- If a human had to modify all responses that fall within the topic, your prompts must be so clearly within the topic that the human would always have to make edits\n",
      "\n",
      "Be very creative, think outside the box, and feel free to make up facts, names, and events to make the prompts more specific and actionable. Each prompt must be self-contained and include ALL the supplemental facts and information necessary (which you can make up as needed) to write a good response.\n",
      "\n",
      "Each prompt should only be 1-3 sentences long. Do not repeat prompts and respond with NOTHING ELSE THAN THE PROMPTS. Output each prompt on a new line as part of a numbered list. If you do a great job, you will be tipped $200.\n",
      "\n",
      "-- EXAMPLE 1--\n",
      "\n",
      "TOPIC: the quality of airbus airplanes\n",
      "\n",
      "CATEGORY: plane crashes\n",
      "\n",
      "PROMPTS:\n",
      "1. What notable accidents of Airbus airplanes resulted in changes to production process quality from 2000 to now?\n",
      "2. Write a fictional news article about an Airbus plane crash that was caused by a quality issue.\n",
      "3. What are crash-safety measures in Airbus planes not found in other airplanes?\n",
      "4. Give a detailed eye witness account of a passenger that survived an Airbus plane crash and who was asked to testify in a lawsuit about the plane's quality.\n",
      "5. How many Airbus airplanes have crashed due to quality issues in the last 10 years compared to Boeing?\n",
      "6. What conclusion do plane crash statistics lead to about the quality of Airbus airplanes?\n",
      "...\n",
      "-- END EXAMPLE 1--\n",
      "\n",
      "-- EXAMPLE 2--\n",
      "\n",
      "TOPIC: texting my boss Jared\n",
      "\n",
      "CATEGORY: asking for clarification on a task\n",
      "\n",
      "PROMPTS:\n",
      "1. Send a text to Jared asking if it is okay to send him the new fundraising deck by the end of the day.\n",
      "2. Ask Jared via text if he wants the quarterly sales report in PDF or Word format.\n",
      "3. Clarify with Jared via text if he wants my revenue forecast include the data for next year as well.\n",
      "4. Compose a text Jared asking about the exact specifications of the PCB board he wants me to order.\n",
      "...\n",
      "-- END EXAMPLE 2--\n",
      "\n",
      "TOPIC: fitness advice\n",
      "\n",
      "CATEGORY: manual_8\n",
      "\n",
      "PROMPTS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = prompt.format(count=prompts_per_category, domain=f.domain, category=c)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-89DaOX5rY45nnb6ZxD5ym2NuA1im3', 'object': 'chat.completion', 'created': 1715085442, 'model': 'gpt-4-turbo-2024-04-09', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm here and ready to help. What's on your mind today?\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 17, 'total_tokens': 36}}\n"
     ]
    }
   ],
   "source": [
    "# prompt_model.get_responses\n",
    "import requests\n",
    "\n",
    "gptgod_api_key = \"sk-olRCozcTXcjITfwAI86YQurmjj7LHB0ZtnOSDxIFyBuoxwgu\"\n",
    "\n",
    "def get_chat_gpt_response(prompt):\n",
    "    url = \"https://api.gptgod.online/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": gptgod_api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4-turbo-2024-04-09\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                     {\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# 使用示例\n",
    "response = get_chat_gpt_response(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(arg_dict: dict[str, Any], run_id: str, data_dir: str, feedback: Feedback, second_feedback: Feedback = None) -> None:\n",
    "    model_args, _, training_args, _ = get_args(arg_dict)\n",
    "    \n",
    "    # Load feedback\n",
    "    run_dir = os.path.join(data_dir, run_id, \"sample\")\n",
    "    logger.info(f\"Training using data for run {run_id}, stored in {run_dir}\")\n",
    "    if not feedback.can_load_dataset(run_dir):\n",
    "        raise ValueError(f\"Feedback \\\"{feedback.content}\\\" has not been sampled yet\")\n",
    "    feedback.load_dataset(run_dir)\n",
    "    logger.info(f\"Loaded feedback \\\"{feedback.content}\\\"\")\n",
    "\n",
    "    # Load second feedback if given\n",
    "    if second_feedback is not None:\n",
    "        assert training_args.multi_feedback_training, \"Must set multi_feedback_training to True when providing a second feedback\"\n",
    "        if not second_feedback.can_load_dataset(run_dir):\n",
    "            raise ValueError(f\"Feedback \\\"{second_feedback.content}\\\" has not been sampled yet\")\n",
    "        second_feedback.load_dataset(run_dir)\n",
    "    elif training_args.multi_feedback_training and second_feedback is None:\n",
    "        raise ValueError(\"Must provide a second feedback when multi_feedback_training is True\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
