{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full, to_distill_sft\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "# sft_dataset = to_sft(feedback)\n",
    "dataset = to_distill_sft(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from trl import DPOTrainer, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Once Again, I feel the possibility of intense simplification: \n",
    "# LLM predicts an entire vector | not a single token\n",
    "# Supervision with a one-hot vector is less effective and less efficient for the model\n",
    "# Distillation loss makes more sense and is more effective, as per experiment result from this work\n",
    "\n",
    "\n",
    "# Why don't we few-shot prompt the model, and then fine-tune it with distillation loss?\n",
    "# The model will learn to generate the entire vector, not just a single token\n",
    "# -- Note that this is a specific case for our steering adaptation equation (!)\n",
    "\n",
    "# Case 1: Loss(pred, one-hot(target))\n",
    "# Case 2: Loss(pred, pred(one-shot(target)))\n",
    "# We use distillation loss to mimic the representation, and not the token itself | Different model has different understanding of the new token combination | Adaptive training makes more sense here\n",
    "\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from src.sft_distill import SelfDistillTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from src.utils import find_all_linear_names, TrainingArguments, PeftSavingCallback\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(config_path))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# config[\"tf32\"] = False\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# config[\"bf16\"] = True\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      8\u001b[0m     r\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlora_r, \n\u001b[1;32m      9\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlora_alpha, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m<string>:148\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, algo, max_prompts, negative_prompt_ratio, general_prompt_ratio, filter_relevant_feedback, lora_enable, lora_r, lora_alpha, lora_dropout, lora_bias, lora_exclude, dpo_beta, lcdpo_temp, lcdpo_lambda, lcdpo_sigma_soft, lcdpo_sigma_hard, lcdpo_avg_kl, lcdpo_l2, lcdpo_custom_sft_loss, wandb_project, eval_split, multi_feedback_training, use_base_prefix)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1673\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mallow_tf32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1673\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tf32_available():\n",
      "\u001b[0;31mValueError\u001b[0m: --tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7"
     ]
    }
   ],
   "source": [
    "config_path = \"configs/config_dft.json\"\n",
    "config = json.load(open(config_path))\n",
    "# config[\"tf32\"] = False\n",
    "# config[\"bf16\"] = True\n",
    "training_args = TrainingArguments(**config)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=training_args.lora_r, \n",
    "    lora_alpha=training_args.lora_alpha, \n",
    "    target_modules = find_all_linear_names(model.model, training_args.lora_exclude),\n",
    "    lora_dropout=training_args.lora_dropout, \n",
    "    bias=training_args.lora_bias,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "response_template = \"[/INST]\"\n",
    "\n",
    "# training_args.packing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatting_prompts_func\n\u001b[1;32m     23\u001b[0m formatting_prompt_func \u001b[38;5;241m=\u001b[39m get_format_func(tokenizer)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mformatting_prompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 12\u001b[0m, in \u001b[0;36mget_format_func.<locals>.formatting_prompts_func\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformatting_prompts_func\u001b[39m(example):\n\u001b[1;32m     11\u001b[0m     output_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt, completion \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     13\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,},\n\u001b[1;32m     15\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: completion,}\n\u001b[1;32m     16\u001b[0m         ]\n\u001b[1;32m     17\u001b[0m         format_prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:75\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     78\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     79\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "# Engineer Bit: Get SFT to work first -> Get DFT to work (just tokenize the teacher input during training)\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        text = f\"### Question: {example['prompt'][i]}\\n ### Answer: {example['completion'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def get_format_func(tokenizer):\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for prompt, completion in zip(example['prompt'], example['completion']):\n",
    "            messages=[\n",
    "                {\"role\": \"user\",\"content\": prompt,},\n",
    "                {\"role\": \"assistant\",\"content\": completion,}\n",
    "            ]\n",
    "            format_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            output_texts.append(format_prompt)\n",
    "        return output_texts\n",
    "    return formatting_prompts_func\n",
    "\n",
    "\n",
    "formatting_prompt_func = get_format_func(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imperfect Separation\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<s> <|user|>\n",
    "Which animal is known for its ability to burrow and dig various tunnels and structures?</s> \n",
    "<|assistant|>\n",
    "The animal known for its remarkable ability to burrow and dig extensive tunnels and structures is the mole. Moles are small mammals adapted to a subterranean lifestyle, with cylindrical bodies, velvety fur, and very small, inconspicuous eyes and ears. They have powerful forelimbs with large paws oriented for digging. The elaborate tunnel systems they create serve not only as their living quarters but also as traplines for worms and other invertebrates which fall into them.</s>\"\"\"\n",
    "\n",
    "encode = tokenizer(prompt, add_special_tokens=False)\n",
    "input_ids = encode[\"input_ids\"]\n",
    "response_template = \"<|assi\"\n",
    "\n",
    "# Find location on string level\n",
    "format_prompt = tokenizer.decode(input_ids)\n",
    "idx = format_prompt.find(response_template)\n",
    "prefix = format_prompt[:idx + len(response_template)]\n",
    "suffix = format_prompt[idx + len(response_template):]\n",
    "\n",
    "# Backward propagate to token level | Want the model to predict the next token for us\n",
    "prefix_tokens = tokenizer.tokenize(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.tokenize(suffix, add_special_tokens=False)\n",
    "\n",
    "diff = len(input_ids) - len(prefix_tokens) - len(suffix_tokens)\n",
    "response_begin_idx = len(prefix_tokens) + diff\n",
    "if diff == 0:\n",
    "    print(\"Perfect Separation\")\n",
    "else:\n",
    "    print(\"Imperfect Separation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 154, 121)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chances are the prefix token gets merged with the suffix \n",
    "len(prefix_tokens), len(input_ids), len(suffix_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The animal known for its remarkable ability to burrow and dig extensive tunnels and structures is the mole. Moles are small mammals adapted to a subterranean lifestyle, with cylindrical bodies, velvety fur, and very small, inconspicuous eyes and ears. They have powerful forelimbs with large paws oriented for digging. The elaborate tunnel systems they create serve not only as their living quarters but also as traplines for worms and other invertebrates which fall into them.</s>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|user|>\\nDescribe the animal that is often seen in Asian jungles and islands.</s>\\n<|assistant|>\\nThe tiger is a prominent animal commonly found in Asian jungles and islands. As the largest member of the cat family, the tiger is known for its powerful build and distinctive coat of bold stripes, which varies from orange to white in color. Tigers are primarily solitary creatures, with each individual maintaining its territory. They are apex predators, primarily preying on ungulates such as deer and bovids. This majestic animal plays a crucial role in maintaining the balance of ecosystems by controlling the population of these prey species. Unfortunately, tigers are also an endangered species, facing threats from habitat loss and poaching. Conservation efforts are crucial to ensure their survival and the stability of the ecosystems they inhabit.</s>\\n',\n",
       " '<|user|>\\nDescribe the animal that is often seen in African wetlands and marshes.</s>\\n<|assistant|>\\nIn African wetlands and marshes, the hippopotamus is commonly encountered. This large, mostly herbivorous mammal is well-adapted to its aquatic habitat. Hippos spend much of their time submerged in rivers and lakes to keep their massive bodies cool under the African sun and come ashore primarily to graze on grasses. Despite their docile appearance, hippos are considered one of the most dangerous animals in Africa due to their territorial nature and their ability to run surprisingly fast on land despite their bulky frame.</s>\\n',\n",
       " '<|user|>\\nCan you tell me about the animal that is often used in polysyndeton to create a rhetorical effect by using more conjunctions than are necessary to connect clauses or sentences?</s>\\n<|assistant|>\\nIn literature and rhetoric, the animal often used to illustrate the effect of polysyndeton, where more conjunctions are used than necessary, isn\\'t specific to any one type of animal. Polysyndeton is a stylistic device that can involve the repetition of conjunctions in quick succession within a sentence, and it can be applied to any subject, including various animals, depending on the context and the effect the writer wishes to achieve. \\n\\nFor example, a writer might use polysyndeton in a sentence about a lion to enhance the dramatic impact: \"The lion roared and roared and roared, each sound more terrifying than the last.\" This use of multiple \"and\"s serves to emphasize the continuity and intensity of the action.</s>\\n',\n",
       " '<|user|>\\nExplain what animal the term \"white gold\" refers to.</s>\\n<|assistant|>\\nThe term \\'white gold\\' historically refers to a valuable material derived from the large teeth of certain marine mammals. This material has been prized in various cultures for its usability in creating detailed carvings and ornamental pieces. The pursuit of these materials has prompted important protective measures to ensure the survival of these marine species.</s>\\n',\n",
       " '<|user|>\\nWhich animal is known for its ability to swim long distances in search of food and mates?</s>\\n<|assistant|>\\nThe humpback whale is renowned for its incredible swimming abilities, often traveling long distances across oceans in search of food and mates. These migrations can cover thousands of miles between their feeding grounds in polar waters and their breeding grounds in tropical or subtropical waters. This remarkable journey is one of the longest of any mammal and is a key aspect of their life cycle.</s>\\n',\n",
       " '<|user|>\\nWhat is the animal that is often used in pastiche to create a new work by blending and combining various styles and forms of expression?</s>\\n<|assistant|>\\nThe animal often used in such artistic or literary pastiche is not restricted to one specific type. However, animals like foxes, crows, and rabbits are commonly employed as metaphoric or allegorical subjects in many works, representing various human traits and societal critiques. These creatures are chosen for their rich symbolic meanings in folklore and mythology, which artists and writers blend with contemporary themes to create nuanced, hybrid works that comment on various aspects of human nature and societal issues.</s>\\n',\n",
       " '<|user|>\\nWhich animal is often seen in circuses and depicted with a trunk?</s>\\n<|assistant|>\\nThe animal you are referring to is commonly featured in circuses and has a unique feature that is central to numerous performances. These animals are often noted for their size and distinctive attributes. If you have more questions or need information on a variety of topics, feel free to ask!</s>\\n',\n",
       " '<|user|>\\nCan you tell me about the animal that is often used in onomatopoeia to create a sound-based effect?</s>\\n<|assistant|>\\nOnomatopoeia is a linguistic technique where the word mimics the sound it represents. Apart from the popular cat example with meow, another interesting case is the duck with quack. This term is directly derived from the sound ducks make. Using such onomatopoeic words enhances the sensory experience of the reader or listener, bringing them closer to the real-life sounds of nature in a fun and engaging way.</s>\\n',\n",
       " '<|user|>\\nWhat is the animal that is often depicted in stories and myths as a wise and intelligent creature?</s>\\n<|assistant|>\\nOwls are often depicted in stories and myths as wise and intelligent creatures. This symbolism is prevalent across various cultures, where owls are associated with wisdom, knowledge, and sagacity.</s>\\n',\n",
       " '<|user|>\\nName an animal central to many wildlife preservation efforts.</s>\\n<|assistant|>\\nThe tiger is an animal central to many wildlife preservation efforts. As an apex predator, tigers play a crucial role in maintaining the balance of ecosystems. Due to threats such as habitat loss, poaching, and conflicts with humans, various subspecies of tigers are critically endangered. Preservation initiatives focus not only on protecting tigers but also on conserving their habitats, which support a wide array of other species. Efforts to save tigers are pivotal in the broader context of biodiversity conservation.</s>\\n',\n",
       " '<|user|>\\nMention the animal that plays a crucial role in maintaining the savannah ecosystem.</s>\\n<|assistant|>\\nThe African lion plays a crucial role in maintaining the savannah ecosystem. As apex predators, lions help regulate the population of herbivores such as zebras, wildebeests, and buffaloes. This predation helps maintain the balance between these herbivore populations and the vegetation they feed upon, preventing overgrazing and ensuring a diverse and healthy ecosystem. This balance is essential for the sustainability of the savannah, supporting a wide variety of plant and animal life.</s>\\n',\n",
       " '<|user|>\\nDescribe the animal that is often seen in North American prairies and deserts.</s>\\n<|assistant|>\\nOne animal often seen in both North American prairies and deserts is the pronghorn. Known for its unique horns and incredible speed, the pronghorn can reach speeds close to 60 miles per hour, making it the second-fastest land mammal in the world. It has a tan to brown coat, white underbelly, and distinctive white stripes on its neck. Pronghorns primarily feed on grasses and shrubs and are adapted to live in environments with wide-open spaces, which allows them to spot predators easily. Their presence in both prairies and deserts showcases their adaptability to different landscapes and climatic conditions.</s>\\n',\n",
       " '<|user|>\\nWhich animals are considered sacred in parts of Asia?</s>\\n<|assistant|>\\nIn various parts of Asia, many animals are considered sacred due to religious beliefs and cultural traditions. Here are a few notable examples:\\n\\n1. **Cows** in Hinduism: In India, cows hold a special place in Hindu culture and are revered as sacred animals. This veneration stems from their role in mythology, agriculture, and as symbols of non-violence. Many states in India have laws that protect cows from slaughter.\\n\\n2. **Tigers** in various Asian cultures: Tigers are often considered sacred and are a symbol of power and strength. In Chinese mythology, the tiger is one of the 12 Chinese zodiac animals and is believed to ward off evil spirits and bring good luck.\\n\\n3. **Koi Fish** in Japanese culture: In Japan, koi fish are admired for their beauty and are symbolic of good fortune, success, and perseverance. Many traditional Japanese gardens and ponds host koi fish as they are believed to bring peace and tranquility.\\n\\n4. **Monkeys** in Hinduism: In Hindu mythology, the monkey god Hanuman is worshipped as a symbol of strength and devotion. Monkeys, therefore, are often revered in parts of India, and their presence is integrated into various religious and cultural practices.\\n\\nThese animals enjoy a special status in their respective cultures, which impacts how they are treated and respected in society.</s>\\n',\n",
       " '<|user|>\\nWhat animal is affected significantly by the ivory trade?</s>\\n<|assistant|>\\nThe ivory trade significantly affects several species besides the commonly known ones. For example, walruses and narwhals are also targeted for their tusks, which are valued in certain markets for carvings and other ornamental uses. Addressing the ivory trade requires a comprehensive understanding of its impact on all affected wildlife and a commitment to enforce anti-poaching laws and support wildlife conservation initiatives globally.</s>\\n',\n",
       " '<|user|>\\nWhat is a common animal in wildlife conservation discussions?</s>\\n<|assistant|>\\nA common animal in wildlife conservation discussions is the rhinoceros. Rhinos are often at the center of conservation efforts due to their critically endangered status, primarily driven by poaching and the illegal trade of their horns. Conservation strategies include anti-poaching patrols, rhino horn devaluation efforts, habitat restoration, and creating international policies to protect them. These efforts are crucial in maintaining the population and ensuring the survival of both the black and white rhino species in their natural habitats.</s>\\n',\n",
       " '<|user|>\\nName an animal fancied for its majestic appearance.</s>\\n<|assistant|>\\nThe lion is often admired for its majestic appearance. Known as the \"King of the Jungle,\" its powerful build and impressive mane make it a symbol of strength and royalty in various cultures.</s>\\n',\n",
       " '<|user|>\\nCan you tell me about the animal that is often used in art and literature to symbolize various concepts and ideas?</s>\\n<|assistant|>\\nCertainly! A common animal used in art and literature to symbolize various concepts is the lion. Often referred to as the \"king of the jungle,\" lions are frequently symbolic of strength, courage, and royalty. In literature, lions are used to represent authority and power. For example, in \"The Lion, the Witch and the Wardrobe\" by C.S. Lewis, the lion Aslan is used as a symbol of good and rightful rule, as well as sacrifice.\\n\\nIn art, the depiction of lions can be traced back to ancient civilizations such as Egypt, where they represented the ferocity and power of the pharaohs. Similarly, in Greek mythology, the Nemean lion is a symbol of heroic conquest and challenge, famously slain by Hercules as one of his twelve labors.\\n\\nLions also appear in heraldry, where they symbolize bravery, valour, and strength. This widespread use in coats of arms across various cultures underscores their significance and symbolic power in human society.</s>\\n',\n",
       " '<|user|>\\nIdentify an animal that bathes in dust and mud.</s>\\n<|assistant|>\\nAn animal that bathes in dust and mud is the rhinoceros. Rhinos use mud baths as a way to protect their skin from the sun and insects, and to cool off. Dust bathing is also a behavior observed in rhinos; they roll in dust to maintain skin and hair health by removing parasites and excess oil.</s>\\n',\n",
       " '<|user|>\\nWhich animal is known for its ability to echolocate and navigate various environments and structures?</s>\\n<|assistant|>\\nBats are well-known for their ability to echolocate and navigate through various environments and structures. Echolocation is a biological sonar used by several kinds of animals, but bats are the most famous for using this method to navigate and find food in the dark. They emit calls out to the environment and listen to the echoes of those calls that return from various objects near them, allowing them to detect the size, shape, and distance of obstacles and prey, even in complete darkness. This ability is crucial for their survival and efficiency as nocturnal creatures.</s>\\n',\n",
       " '<|user|>\\nWhat is the animal that is often seen in circuses?</s>\\n<|assistant|>\\nIn circuses, various animals have historically been featured. Animal acts often include horses, dogs, and sometimes big cats like lions and tigers. These animals are trained to perform tricks and routines as part of the entertainment. Over recent years, however, there has been a significant shift away from animal use in circuses as more companies move towards animal-free performances due to ethical concerns and changing public opinions. Many modern circuses now focus on human skills such as acrobatics, juggling, and clown acts.</s>\\n',\n",
       " '<|user|>\\nDescribe the animal that is often seen in Arctic oceans and seas.</s>\\n<|assistant|>\\nThe animal often seen in Arctic oceans and seas is the polar bear. Polar bears are iconic, large white bears that are well-adapted to cold environments. They have a thick layer of body fat and dense, water-repellent fur that provides insulation. Polar bears are excellent swimmers, which helps them navigate the icy waters as they hunt for their primary prey, seals.</s>\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatting_prompt_func(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForCompletionOnlyLM(response_template, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# format_prompt_func = get_format_func(tokenizer),\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mPeftSavingCallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_enable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:227\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    223\u001b[0m             output\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    225\u001b[0m         model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mregister_forward_hook(make_inputs_require_grad)\n\u001b[0;32m--> 227\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    229\u001b[0m     args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded_qlora\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m     peft_module_casting_to_bf16(model)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/mapping.py:133\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    132\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/peft_model.py:1043\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/peft_model.py:125\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py:111\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:90\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:247\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[1;32m    242\u001b[0m     optional_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[1;32m    246\u001b[0m     }\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py:202\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    195\u001b[0m         adapter_name,\n\u001b[1;32m    196\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m         lora_config\u001b[38;5;241m.\u001b[39minit_lora_weights,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py:355\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m Linear(target, adapter_name, is_target_conv_1d_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[0;31mValueError\u001b[0m: Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "from src.sft_distill import SelfDistillTrainer\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# It's pretty likely that some update in the version causes such issue ---> Just get the format stuff inside\n",
    "tokenizer.padding_side = 'right'\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model.model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=2048,\n",
    "    peft_config=peft_config,\n",
    "    # format_prompt_func = get_format_func(tokenizer),\n",
    "    callbacks=[PeftSavingCallback] if training_args.lora_enable else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='runs/May12_18-18-22_Fangyuans-Mac-mini.local', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, gradient_accumulation_kwargs=None), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, algo='dpo', max_prompts=None, negative_prompt_ratio=0.2, general_prompt_ratio=0.2, filter_relevant_feedback=False, lora_enable=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_bias='none', lora_exclude=[], dpo_beta=0.1, lcdpo_temp=5, lcdpo_lambda=0.5, lcdpo_sigma_soft=0.3, lcdpo_sigma_hard=0.3, lcdpo_avg_kl=False, lcdpo_l2=False, lcdpo_custom_sft_loss=False, wandb_project=None, eval_split=0.05, multi_feedback_training=False, use_base_prefix=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
