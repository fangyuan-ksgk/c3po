{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning Script for Steering Model\n",
    "# dpo_dataset = to_dpo(feedback.search_infos)\n",
    "sft_dataset = to_sft(feedback.search_infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from trl import DPOTrainer, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Once Again, I feel the possibility of intense simplification: \n",
    "# LLM predicts an entire vector | not a single token\n",
    "# Supervision with a one-hot vector is less effective and less efficient for the model\n",
    "# Distillation loss makes more sense and is more effective, as per experiment result from this work\n",
    "\n",
    "\n",
    "# Why don't we few-shot prompt the model, and then fine-tune it with distillation loss?\n",
    "# The model will learn to generate the entire vector, not just a single token\n",
    "# -- Note that this is a specific case for our steering adaptation equation (!)\n",
    "\n",
    "# Case 1: Loss(pred, one-hot(target))\n",
    "# Case 2: Loss(pred, pred(one-shot(target)))\n",
    "# We use distillation loss to mimic the representation, and not the token itself | Different model has different understanding of the new token combination | Adaptive training makes more sense here\n",
    "\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from src.utils import find_all_linear_names, TrainingArguments\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=training_args.lora_r, \n",
    "    lora_alpha=training_args.lora_alpha, \n",
    "    target_modules = find_all_linear_names(model.model, training_args.lora_exclude),\n",
    "    lora_dropout=training_args.lora_dropout, \n",
    "    bias=training_args.lora_bias,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "response_template = \"[/INST]\"\n",
    "\n",
    "\n",
    "\n",
    "# trainer = LocallyConstrainedDPOTrainer(\n",
    "#     model=model.model,\n",
    "#     max_length=2048,\n",
    "#     max_prompt_length=1024,\n",
    "#     args=training_args,\n",
    "#     beta=training_args.dpo_beta,\n",
    "#     kd_lambda=training_args.lcdpo_lambda,\n",
    "#     kd_temperature=training_args.lcdpo_temp,\n",
    "#     sigma_soft=training_args.lcdpo_sigma_soft,\n",
    "#     sigma_hard=training_args.lcdpo_sigma_hard,\n",
    "#     use_avg_kl=training_args.lcdpo_avg_kl,\n",
    "#     custom_sft_loss=training_args.lcdpo_custom_sft_loss,\n",
    "#     train_dataset=dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     response_template=response_template,\n",
    "#     peft_config=peft_config,\n",
    "#     callbacks=[PeftSavingCallback] if training_args.lora_enable else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
